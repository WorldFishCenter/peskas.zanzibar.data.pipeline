url: https://worldfishcenter.github.io/peskas.zanzibar.data.pipeline/
template:
  bootstrap: 5
  bslib:
    primary: "#47475C"
    heading_font:
      google:
        family: Alegreya Sans SC
        wght: 400
    base_font:
      google:
        family: IBM Plex Sans
    code_font:
      google: Fira Mono

reference:
  - title: Data Pipeline Workflow
    desc: >
      Core functions that execute each step in the data pipeline, from data ingestion
      through validation, analysis, and export to MongoDB and cloud storage.
    contents:
      - has_keyword("workflow")

  - title: Data Ingestion
    desc: >
      Functions for pulling data from external sources (KoboToolbox, Pelagic Data Systems)
      and transforming it into standardized formats.
    contents:
      - has_keyword("ingestion")

  - title: Cloud Storage Management
    desc: >
      Functions for interacting with cloud storage providers (Google Cloud Storage, MongoDB),
      uploading, downloading, and managing data files in various formats.
    contents:
      - has_keyword("storage")

  - title: Data Preprocessing
    desc: >
      Functions for cleaning, transforming, and structuring raw data into standardized
      formats ready for analysis, including data nesting, reshaping, and trip processing.
    contents:
      - has_keyword("preprocessing")

  - title: Data Mining & Summarization
    desc: >
      Functions for enriching fisheries data with scientific information, taxonomic
      classification, biological parameters, and creating summary datasets for analysis.
    contents:
      - has_keyword("mining")
      - has_keyword("summary")

  - title: Data Modeling & Analysis
    desc: >
      Functions for statistical modeling, fleet activity estimation, and scaling
      sample-based GPS data to fleet-wide estimates using boat registry information.
    contents:
      - has_keyword("modeling")
      - has_keyword("analysis")

  - title: Data Validation
    desc: >
      Functions for validating fisheries data through quality checks, statistical
      outlier detection, and applying domain-specific validation rules.
    contents:
      - has_keyword("validation")

  - title: Data Export & Visualization
    desc: >
      Functions for exporting processed data to MongoDB collections, creating
      geographic visualizations, and preparing data for portals and reporting.
    contents:
      - has_keyword("export")
      - kepler_mapper

  - title: Pipeline Orchestration
    desc: >
      High-level functions that orchestrate complete analysis pipelines, combining
      multiple processing steps into integrated workflows.
    contents:
      - has_keyword("pipeline")

  - title: Helper Functions
    desc: >
      Utility functions that support the main pipeline operations, providing
      common data manipulation and processing capabilities.
    contents:
      - has_keyword("helper")